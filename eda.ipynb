{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataloader import GraphDataset, GraphTextDataset, TextDataset, GraphTextInMDataset\n",
    "import networkx as nx\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "source_directory = r\"C:\\Antoine\\Study\\Master 2 - MVA\\ALTEGRAD\\Challenge\\Public\\Public\\data\"\n",
    "destination_directory = r\"C:\\Antoine\\Study\\Master 2 - MVA\\ALTEGRAD\\Altegrad-MVA-2023-2024\\data\"\n",
    "\n",
    "def move_files(source_directory, destination_directory):\n",
    "    # Make sure the destination directory exists, create it if not\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    # Get the total number of files to track progress\n",
    "    total_files = sum([len(files) for _, _, files in os.walk(source_directory)])\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    progress_bar = tqdm(total=total_files, desc=\"Moving files\", unit=\"file\")\n",
    "\n",
    "    # Walk through the source directory and its subdirectories\n",
    "    for root, _, files in os.walk(source_directory):\n",
    "        for file_name in files:\n",
    "            source_path = os.path.join(root, file_name)\n",
    "            # Create the corresponding subdirectory structure in the destination\n",
    "            relative_path = os.path.relpath(source_path, source_directory)\n",
    "            destination_path = os.path.join(destination_directory, relative_path)\n",
    "\n",
    "            destination_dir = os.path.dirname(destination_path)\n",
    "            if not os.path.exists(destination_dir):\n",
    "                os.makedirs(destination_dir)\n",
    "\n",
    "\n",
    "            # Move the file\n",
    "            shutil.move(source_path, destination_path)\n",
    "\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_files(source_directory, destination_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"data/processed/train/\"\n",
    "test_dir = r\"data/processed/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.load(train_dir + \"data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(tokenizer: AutoTokenizer):\n",
    "    gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "    val_dataset = GraphTextInMDataset(\n",
    "        root=\"./data/\", gt=gt, split=\"val\", tokenizer=tokenizer\n",
    "    )\n",
    "    train_dataset = GraphTextInMDataset(\n",
    "        root=\"./data/\", gt=gt, split=\"train\", tokenizer=tokenizer\n",
    "    )\n",
    "    return val_dataset, train_dataset\n",
    "\n",
    "val_dataset, train_dataset = load_datasets(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample):\n",
    "    edges = sample.edge_index\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for i in range(len(edges[0])):\n",
    "        G.add_edge(int(edges[0][i]), int(edges[1][i]))\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, font_size=8, node_color='skyblue', node_size=200, edge_color='gray', linewidths=0.3, arrows=False)\n",
    "\n",
    "    # Show the plot\n",
    "    decoded_input = tokenizer.batch_decode(sample.input_ids, skip_special_tokens=True)[0]\n",
    "    wrapped_text = textwrap.fill(decoded_input, width=70)\n",
    "\n",
    "    plt.text(0, -1, wrapped_text, ha='center', va='center', fontsize=8, bbox=dict(facecolor='white', alpha=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "show_sample(train_dataset[41])\n",
    "plt.subplot(2, 2, 2)\n",
    "show_sample(train_dataset[121])\n",
    "plt.subplot(2, 2, 3)\n",
    "show_sample(train_dataset[452])\n",
    "plt.subplot(2, 2, 4)\n",
    "show_sample(train_dataset[71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\Antoine/.cache\\torch\\sentence_transformers\\microsoft_BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_input = tokenizer.batch_decode(train_dataset[41].input_ids, skip_special_tokens=True)[0]\n",
    "print(decoded_input)\n",
    "embedding = model.encode(\n",
    "    decoded_input\n",
    ")\n",
    "print(embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

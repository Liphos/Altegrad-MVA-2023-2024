{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning.losses import NTXentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(v1, v2):\n",
    "    logits = torch.matmul(v1, torch.transpose(v2, 0, 1))\n",
    "    labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "    return CE(logits, labels) + CE(torch.transpose(logits, 0, 1), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37.6935)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "labels must be a 1D tensor of shape (batch_size,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_88936\\2241724223.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNTXentLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Antoine\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Antoine\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_metric_learning\\losses\\base_metric_loss_function.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, embeddings, labels, indices_tuple, ref_emb, ref_labels)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mc_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Antoine\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_metric_learning\\utils\\common_functions.py\u001b[0m in \u001b[0;36mcheck_shapes\u001b[1;34m(embeddings, labels)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of embeddings must equal number of labels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"labels must be a 1D tensor of shape (batch_size,)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: labels must be a 1D tensor of shape (batch_size,)"
     ]
    }
   ],
   "source": [
    "# Create two random batch embeddings\n",
    "batch_size = 16\n",
    "embedding_size = 128\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "v1 = torch.randn(batch_size, embedding_size)\n",
    "v2 = torch.randn(batch_size, embedding_size)\n",
    "\n",
    "\n",
    "# Compute InfoNCE loss\n",
    "loss = contrastive_loss(v1, v2)\n",
    "print(loss)\n",
    "\n",
    "labels = torch.arange(batch_size, device=v1.device)\n",
    "loss = NTXentLoss()(v1, v2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss = NTXentLoss()\n",
    "temperature = 1.0\n",
    "logits = torch.matmul(v1, v2.t()) / temperature\n",
    "labels = torch.diagonal(logits)\n",
    "\n",
    "loss(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_nce_loss = NTXentLoss()\n",
    "\n",
    "def info_nce_loss__(v1, v2, temperature=0.7):\n",
    "    logits = torch.matmul(v1, torch.transpose(v2, 0, 1))\n",
    "\n",
    "    # Generate labels\n",
    "    labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "\n",
    "    # Cross-Entropy Loss for positive pairs\n",
    "    ce_positive = CE(logits, labels)\n",
    "\n",
    "    # Cross-Entropy Loss for negative pairs\n",
    "    ce_negative = CE(torch.transpose(logits, 0, 1), labels)\n",
    "\n",
    "    # Compute mutual information\n",
    "    prob_positive = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
    "    prob_negative = torch.nn.functional.softmax(torch.transpose(logits, 0, 1) / temperature, dim=-1)\n",
    "\n",
    "    mi_loss = -torch.sum(prob_positive * torch.log(prob_positive / prob_negative)) / logits.shape[0]\n",
    "\n",
    "    # InfoCNE Loss\n",
    "    infocne_loss = ce_positive + ce_negative + mi_loss\n",
    "\n",
    "    return infocne_loss\n",
    "\n",
    "def infoCNE_loss(v1, v2, temperature=0.7):\n",
    "    # Normalize embeddings\n",
    "    v1_normalized = F.normalize(v1, dim=-1, p=2)\n",
    "    v2_normalized = F.normalize(v2, dim=-1, p=2)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sim_matrix = torch.matmul(v1_normalized, v2_normalized.t()) / temperature\n",
    "\n",
    "    # Compute positive pair similarity (diagonal of the similarity matrix)\n",
    "    pos_pair_sim = torch.diagonal(sim_matrix)\n",
    "\n",
    "    # Compute negative pair similarity\n",
    "    neg_pair_sim = torch.logsumexp(sim_matrix, dim=1) - pos_pair_sim\n",
    "\n",
    "    # Compute InfoCNE loss\n",
    "    loss = -torch.mean(pos_pair_sim - neg_pair_sim)\n",
    "\n",
    "    return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
